# Myocardial Infarction Complications Classifier

Our goal for this project was to classify the possible complications of a myocardial infarction (MI) or heart attack based on various patient attributes. The dataset we will be using is from the UCI Machine Learning Repository, and it contains data from 1992-1995 for 1700 patients in Krasnoyarsk, Russia. There are 111 patient attributes and 12 possible MI complications included in the dataset. The techniques that will be used for our classification models are Random Forest and KNN. After creating two different classification models, we will compare and analyze them. We decided which complications to analyze by showing a graph displaying each complication's frequency.

The data set was cleaned before performing any exploratory data analysis. Five columns with greater than 50% NA values were dropped because they would not be useful to our data analysis. Missing values for the remaining numerical variables were filled in using mean imputation, and missing values for categorical variables were filled in using mode imputation. Random Forest and KNN were the unsupervised learning methods used to classify possible complications of MI. The feature variables used for this were chosen by selecting the input variable with the greatest Pearson’s correlation with a given complication. K-means clustering was also used to visualize groupings in our data.

The pairwise Pearson’s correlation was also calculated for all columns in the dataset, excluding the ID of a patient. We used these values to determine the input variable that had the greatest correlation with our target variables. Although these had the greatest correlation with our target variables, all except for the correlation between OTEK_LANC and NITR_S were below 0.3, which can be interpreted as very little correlation. Nevertheless, we used these input variables in our classification models, as other variables had even smaller Pearson’s correlation values.

We used a Random Forest model to classify the top five most frequent MI complications in our data set: chronic heart failure (ZSN), atrial fibrillation (FIBR_PREDS), pulmonary edema (OTEK_LANC), relapse of MI (REC_IM), and post-infarction angina (P_IM_STEN). The model was trained using the features with the highest Pearson’s correlation coefficient with our five MI complications of interest. These features are the presence of chronic heart failure in the anamnesis (ZSN_A), paroxysms of atrial fibrillation on ECG at the time of hospital admission (n_r_ecg_p_0), sse of liquid nitrates in the ICU (NITR_S), relapse of the pain in the third day of the hospital period (R_AB_3_n), and the functional class of angina pectoris in the last year (FK_STENOK). The test size was 30%, and the training size was 70%, with a random state of 50 to ensure the code reproduces the same result each time it is run. After training the model, we output each model's accuracy, precision, recall, and confusion matrix.

Overall, the classification models using Random Forest and our selected feature variables did not yield good results for classifying MI complications. They all had high accuracy but overall low precision and recall, except for the model for ZSN, which had a high precision. The high accuracy values for all models are due to the fact that a large portion of the test and training data included patients who did not have the complication, which the models correctly predicted. However, the goal was to classify the complications, so accuracy is not a good measure of how well the models perform. Instead, the focus should be on obtaining a high recall, which is preferred for classifying what complication a patient might have after an MI. We also used KNN classification for the MI complications we were interested in, and they resulted in values similar to those of the Random Forest classification models.

Regarding accuracy, the Random Forest and KNN models had very similar values. The accuracy of the KNN model for ZSN was greater by 0.01, and the KNN model for FIBR_PREDS was greater by 0.02. The models for both classification methods had the same accuracy for OTEK_LANC, REC_IM, and P_IM_STEN. The precision and recall for both types of classification models are also similar, but the Random Forest model for OTEK_LANC does not have precision and recall values of 0.00, whereas the KNN model does. Since this is the only distinct difference between the two classification methods, the Random Forest models are slightly better in classifying an MI complication. A possible reason for the small precision and recall values is the selection of input variables, which had the largest correlation with the target variables overall but still needed to be larger to be statistically significant.

We used K-Means Clustering to cluster each MI complication using the same feature variables we used for Random Forest classification. To use K-means on our data, we utilized PCA as a dimensionality reduction technique to reduce the number of dimensions to 2 principal components. The silhouette scores for ZSN, FIBR_PREDS, OTEK_LANC, REC_IM, and P_IM_STEN are 0.905, 0.953, 0.947, 0.950, and 0.948, respectively. The high silhouette scores show that the clusters in each plot are well-separated. This also means that the points in each cluster are closer to each other than they are to points in other clusters.

## Conclusion
After cleaning the data and performing EDA, calculating Pearson’s correlation showed that ZSN_A was most highly correlated with ZSN, n_r_ecg_p_05 with FIBR_PREDS, NITR_S with OTEK_LANC, R_AB_3_n with REC_IM, and FK_STENOK with P_IM_STEN, so they were selected as our input variables. However, both methods of creating classification models using the input variables as features yielded similar results of high accuracy and low precision and recall. These low values might indicate that the input variables we selected did not significantly determine an MI complication, which resulted in our classification models performing poorly. The Random Forest models were more favorable between the two methods due to their non-zero precision and recall values for three of the five models. K-Means clustering was also performed on a subset of the MI data set, including the same columns used for classification. We were able to visualize clusters for each target variable and obtain high silhouette scores. Overall, we were able to create classification models with high accuracy and use K-Means clustering to create well-defined clusters, but the models can be improved to have higher precision and recall values.
